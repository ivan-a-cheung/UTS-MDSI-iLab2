{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Google News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/h_d_puckeridge/.pyenv/versions/3.10.11/envs/ilab2/lib/python3.10/site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/h_d_puckeridge/.pyenv/versions/3.10.11/envs/ilab2/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "# extract urls from google news search query\n",
    "def search_google_news_urls(search_query):\n",
    "    hrefs = []\n",
    "    page = 1\n",
    "    query_end = False\n",
    "    while query_end==False:\n",
    "        search_url = f'https://www.google.com/search?q={search_query}&source=lnms&tbm=nws&start={(page-1)*10}&tbs=sbd:1&safe=active&ssui=on'\n",
    "        r = requests.get(search_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        atags = soup.find_all('a')\n",
    "        links = [link['href'] for link in atags]\n",
    "        hrefs += links\n",
    "        page += 1 \n",
    "        if re.search('Next',str(soup))==None:\n",
    "            query_end = True\n",
    "        print()\n",
    "    return hrefs \n",
    "# exclude urls if they contain an exclusion term\n",
    "def exclude_urls(urls, exclude_list):\n",
    "    val = []\n",
    "    for url in urls: \n",
    "        if 'https://' in url and not any(exclude_word in url for exclude_word in exclude_list):\n",
    "            res = re.findall(r'(https?://\\S+)', url)[0].split('&')[0]\n",
    "            val.append(res)\n",
    "    return list(set(val))\n",
    "# extract full article text\n",
    "def extract_articles(URLs):\n",
    "    articles = []\n",
    "    for url in URLs: \n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text = [paragraph.text for paragraph in paragraphs]\n",
    "        words = ' '.join(text).split(' ')\n",
    "        article = ' '.join(words)\n",
    "        articles.append(article)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "topic_terms = ['quantum','hydrogen','robotics','nanomaterials'] # list of topic terms used to create each query\n",
    "start_date = '2023-08-01'\n",
    "end_date = '2023-08-02'\n",
    "search_conditions = f'+AND+(investment|start-up|invent|development|market|funding|research)+after:{start_date}+before:{end_date}' # search conditions to apply to all queries\n",
    "queries = {topic:topic+search_conditions for topic in topic_terms}  # terms and conditions combined\n",
    "exclude_list = ['maps', 'policies', 'preferences', 'accounts', 'support', 'www.google.com']   # urls containing any of these terms will be excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract URLs from search pages\n",
    "raw_urls = {query:search_google_news_urls(query) for query in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only URLs of search results\n",
    "cleaned_urls = {query:exclude_urls(raw_urls[query], exclude_list) for query in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract full text of articles\n",
    "articles = {query:extract_articles(cleaned_urls[query]) for query in queries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as CSV\n",
    "df.to_csv('data/raw/google_news.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
